<script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.js"></script>
<script src="https://cdn.jsdelivr.net/npm/@ricky0123/vad-web@0.0.7/dist/bundle.min.js"></script>
<script>
  async function main() {
    const myvad = await vad.MicVAD.new({
      onSpeechStart: () => {
        console.log("speech start");
      },
      onSpeechEnd: (audio) => {
        console.log("speech end");
        // do something with `audio` (Float32Array of audio samples at sample rate 16000)...
        const audioBlob = float32ArrayToWav(audio, 16000);
        sendAudio(audioBlob);
      },
    });
    myvad.start();
  }
  main();

  function float32ArrayToWav(audioData, sampleRate) {
    const buffer = new ArrayBuffer(44 + audioData.length * 2);
    const view = new DataView(buffer);

    function writeString(view, offset, string) {
      for (let i = 0; i < string.length; i++) {
        view.setUint8(offset + i, string.charCodeAt(i));
      }
    }

    // RIFF header
    writeString(view, 0, "RIFF");
    view.setUint32(4, 32 + audioData.length * 2, true);
    writeString(view, 8, "WAVE");

    // fmt chunk
    writeString(view, 12, "fmt ");
    view.setUint32(16, 16, true);
    view.setUint16(20, 1, true); // PCM format
    view.setUint16(22, 1, true); // mono
    view.setUint32(24, sampleRate, true);
    view.setUint32(28, sampleRate * 2, true);
    view.setUint16(32, 2, true);
    view.setUint16(34, 16, true);

    // data chunk
    writeString(view, 36, "data");
    view.setUint32(40, audioData.length * 2, true);

    const volume = 1;
    let index = 44;
    for (let i = 0; i < audioData.length; i++) {
      view.setInt16(index, audioData[i] * (0x7fff * volume), true);
      index += 2;
    }

    return new Blob([view], { type: "audio/wav" });
  }

  // Usage:
  const audioBlob = float32ArrayToWav(audio, 16000);

  async function encodeAudioDataToWebM(audio) {
    return new Promise(async (resolve) => {
      const audioContext = new AudioContext({ sampleRate: 16000 });
      const audioBuffer = audioContext.createBuffer(1, audio.length, 16000);
      audioBuffer.copyToChannel(audio, 0);

      const dest = audioContext.createMediaStreamDestination();
      const sourceNode = audioContext.createBufferSource();
      sourceNode.buffer = audioBuffer;
      sourceNode.connect(dest);
      sourceNode.start();

      const mediaRecorder = new MediaRecorder(dest.stream);
      const chunks = [];

      mediaRecorder.ondataavailable = (event) => {
        chunks.push(event.data);
      };

      mediaRecorder.onstop = () => {
        const blob = new Blob(chunks, { type: "audio/webm" });
        resolve(blob);
      };

      mediaRecorder.start();
      await new Promise((resolve) =>
        setTimeout(resolve, audioBuffer.duration * 1000)
      );
      mediaRecorder.stop();
    });
  }

  async function sendAudio(blob) {
    try {
      let token = localStorage.getItem("access-token");
      const url = window.SERVER_URL+"" + "/audio/transcript";
      const formData = new FormData();
      const testID = "2HPBABTLLHGJ75UFSKDKTB422M======"; //임시로 넣은 testID
      formData.append("file", blob);
      formData.append("meetingId", testID);
      const response = await fetch(url, {
        method: "POST",
        body: formData,
        headers: { Authorization: "Bearer " + token },
      });

      if (!response.ok) {
        throw new Error(`HTTP error! status: ${response.status}`);
      }
      const data = await response.json();
      console.log(data.text);
    } catch (error) {
      console.error("Error sending audio", error);
    }
  }
</script>
